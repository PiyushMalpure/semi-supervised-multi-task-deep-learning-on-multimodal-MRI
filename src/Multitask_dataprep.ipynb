{"cells":[{"cell_type":"markdown","metadata":{"id":"ZHdKJK3VYI3r"},"source":["\n","# Multitask Data Preparation\n","This file prepares the data for further use for model training. It will create numpy array of the data with proper train test splits.\n","\n","## Requirements : \n","| Library | version | version name |\n","| :---        |    :----:   |   ------:  |\n","| cudatoolkit |   9.0 | h13b8566_0 |\n","| cudnn |                     7.6.5 |                cuda9.0_0 |  \n","|ipykernel|                 5.3.4|            py37h5ca1d4c_0|    \n","|ipython |                  7.18.1|           py37h5ca1d4c_0|    \n","|jupyter_client|            6.1.7|                      py_0|    \n","|jupyter_core|              4.6.3|                    py37_0|    \n","|keras-applications|        1.0.8|                      py_1|  \n","|keras-preprocessing|       1.1.0|                      py_1 | \n","|matplotlib|                3.3.3|                    pypi_0|    \n","|matplotlib-base|           3.3.2|            py37h817c723_0|  \n","|nibabel|                   3.2.1|                    pypi_0|    \n","|numpy|                     1.19.2|           py37h54aff64_0|\n","|opencv|                    3.4.2|            py37h6fd60c2_1|  \n","|pandas|                    1.1.3|            py37he6710b0_0|  \n","|pillow|                    8.0.1|            py37he98fc37_0|  \n","|py-xgboost|                0.90|             py37he6710b0_1|    \n","|python|                    3.7.9|                h7579374_0|  \n","|scikit-image     |         0.17.2|                   pypi_0|    \n","|scikit-learn     |         0.23.2|           py37h0573a6f_0|    \n","|scipy            |         1.5.2  |          py37h0b6359f_0|  \n","|seaborn          |         0.11.0 |                    py_0|  \n","|tensorboard     |          1.14.0 |          py37hf484d3e_0|  \n","|tensorflow     |           1.14.0 |         gpu_py37hae64822_0|  \n","|tensorflow-gpu|            1.14.0 |              h0d30ee6_0|  \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GTiiew4mXjv7"},"outputs":[],"source":["import pandas as pd\n","import os\n","import pathlib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import tensorflow as tf\n","\n","from tensorflow import keras\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","\n","from tensorflow.keras import backend as K\n","import nibabel as nib\n","import cv2\n","import time\n","from skimage.transform import resize\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split, StratifiedShuffleSplit, StratifiedKFold\n","from tensorflow.keras.utils import to_categorical"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x15ycffcXjv9"},"outputs":[],"source":["from prep_data import get_roi, get_all_subjects, get_subject, get_subject_list, pad_to_shape, remove_padding\n","from PatchGenerator import PatchGenerator"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fMVmZMDEXjv9"},"outputs":[],"source":["from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, CSVLogger, EarlyStopping, TensorBoard\n","from tensorflow.keras.applications.resnet50 import preprocess_input\n","import models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"p_a41cvWXjv9"},"outputs":[],"source":["# Importing segmentation masks and original data after preprocessing\n","basepath = '../data/Step7-Segmentations/' "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OHuh9f_eXjv-"},"outputs":[],"source":["paths = pathlib.Path.iterdir(basepath)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nime4aRsXjv-"},"outputs":[],"source":["paths1 = os.scandir(basepath) #scanning the directory for the subject id forlders present"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aoS_riYfXjv_"},"outputs":[],"source":["\"\"\"\n","Makinig an array of all the IDs available after the segmentation of the data\n","\"\"\"\n","id1 = []\n","basepath = pathlib.Path(basepath)\n","for entry in basepath.iterdir():\n","    if entry.is_dir():\n","        print(entry.name)\n","        id1.append(entry.name)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"439fzORgXjwB"},"outputs":[],"source":["id1.sort()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FHU-5YKrXjwC"},"outputs":[],"source":["len(id1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qFCfSPh7XjwC"},"outputs":[],"source":["\"\"\"\n","Creating dataframe with all the file paths which will then be stored with all the important data\n","as a csv file for future use \n","\"\"\"\n","\n","dfn_T1ce = []\n","dfn_Flair = []\n","dfn_T2 = []\n","dfn_Mask = []\n","for id in id1:\n","    temp = '../Nimhans_data/Step6-Mask_Multiplication/' + str(id) + '/T1WCE_bet.nii.gz'\n","    dfn_T1ce += [temp]\n","    temp1 = '../Nimhans_data/Step6-Mask_Multiplication/' + str(id) + '/' + str(id) + '_mul_FLAIR.nii.gz'\n","    dfn_Flair += [temp1]\n","    temp2 = '../Nimhans_data/Step6-Mask_Multiplication/' + str(id) + '/'+ str(id) +'_mul_T2W.nii.gz'\n","    dfn_T2 += [temp2]\n","    temp3 = basepath + str(id) +'/Pred_Mask_Segm.nii.gz'\n","    dfn_Mask += [temp3]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WErnRjfJXjwD"},"outputs":[],"source":["d = {'Patient ID':id1, 'T1ce':dfn_T1ce, 'FLAIR': dfn_Flair, 'T2': dfn_T2, 'Mask': dfn_Mask}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y3rZCggVXjwD"},"outputs":[],"source":["temp_df = pd.DataFrame(d, columns = ['Patient ID', 'T1ce', 'FLAIR', 'T2', 'Mask'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LoLXRRWWXjwD"},"outputs":[],"source":["temp_df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h8sUd3TIXjwG"},"outputs":[],"source":["temp_df.to_csv('TCGA_new_data2_path.csv') #saving final data as a csv "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mGVx4Q7FXjwG"},"outputs":[],"source":["df_train = temp_df\n","Labels_train = np.zeros(len(df_train))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eWhoncEwXjwH"},"outputs":[],"source":["df_train.set_index('Patient ID', inplace = True)"]},{"cell_type":"markdown","metadata":{"id":"cOKSlodZcRFU"},"source":["### Image preprocessing\n","\n","Code cells below will now process our split data and give us numpy arrays.\n","The following preprocessing is done on the data:\n","1. Data is converted to numpy array\n","2. Slices are taken over the 3D image\n","3. z2 normalization is performed over each slice\n","4. A bounding box is taken over each slice for the area around the mask\n","5. The masked box is then taken over the slice and only the tumor part is considered\n","6. The bounded slices are then stacked over each other also labels are also stacked in the same order as that of the slices.\n","7. Final numpy arrays are then returned and saved for further use in model training"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fPvRYzNCXjwH","scrolled":true},"outputs":[],"source":["\"\"\"\n","    The code below uses the function get_all_subjects from our prep_data.py file. \n","    This gives us the numpy array of normalised images for every subject from the test data. \n","    Similar process will be done for train data.\n","\"\"\"\n","\n","FLAIR_train_data_list, train_mask_list = get_all_subjects(df_train,'FLAIR','Mask',label_values=[1,2,4],transpose_axes=[2,0,1],norm_type='zscore')\n","T1ce_train_data_list, _ = get_all_subjects(df_train,'T1ce','Mask',label_values=[1,2,4],transpose_axes=[2,0,1],norm_type='zscore')\n","T2_train_data_list, _ = get_all_subjects(df_train,'T2','Mask',label_values=[1,2,4],transpose_axes=[2,0,1],norm_type='zscore')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZKfc9ZDpXjwH"},"outputs":[],"source":["start = time.time()\n","\n","number_of_slices = 15 #number of slices to selected\n","\n","X_train_orig = []\n","Y_train_orig = []\n","modality_list = []\n","\n","for idx, msk in enumerate(train_mask_list):\n","    FLAIR_img = FLAIR_train_data_list[idx]\n","    T1ce_img = T1ce_train_data_list[idx]\n","    T2_img = T2_train_data_list[idx]    \n","    label = Labels_train[idx]\n","    \n","    print('Class: ',label,'\\tName : ',df_train.index[idx])\n","    hMin, hMax, wMin, wMax, dMin, dMax = get_roi(msk,10)    \n","    \n","    ## important Axial slices\n","    roi_areas = [(slc,area) for slc,area in enumerate(np.sum(msk,axis=(1,2)))]\n","    roi_areas = sorted(roi_areas,key=lambda x: x[1],reverse=True)\n","    imp_slices = [x for x,_ in roi_areas[0:number_of_slices]]\n","    \n","    for slc in imp_slices:\n","        \n","        modalities = ['FLAIR','T1ce','T2']\n","        modality_mapping = {'FLAIR':1,'T1ce':2,'T2':3}\n","        data_dict = {'FLAIR':FLAIR_img,'T1ce':T1ce_img,'T2':T2_img}\n","        \n","        for mod in modalities:\n","            # for each slice and its respective mask the bounded image is created and are stacked                            \n","            tmp_slice = data_dict[mod][slc]\n","            tmp_slice = data_dict[mod][slc][wMin:wMax,dMin:dMax] # taking bounding box across tumor in slice\n","            tmp_slice = resize(tmp_slice,[128,128],anti_aliasing=True) # resizing to required dimensions               \n","            \n","            modality_list.append(modality_mapping[mod])\n","            Y_train_orig.append(label)\n","            X_train_orig.append(tmp_slice)        \n","        \n","end = time.time()\n","print(end-start)\n","\n","X_train_orig = np.stack(X_train_orig, axis=0)\n","X_train_orig = np.expand_dims(X_train_orig, -1)\n","\n","Y_train_orig = np.stack(Y_train_orig,axis=0)\n","print('training : ',np.unique(Y_train_orig,return_counts=True))\n","Y_train = to_categorical(Y_train_orig,num_classes=None)\n","\n","print(X_train_orig.shape,Y_train.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MDu_NWM0XjwI"},"outputs":[],"source":["del(FLAIR_train_data_list) # save memory\n","del(T2_train_data_list) # save memory\n","del(T1ce_train_data_list) # save memory\n","del(train_mask_list) # save memory"]},{"cell_type":"markdown","metadata":{"id":"BOWA3HRqhGx6"},"source":["Below code saves all the data in numpy format, including signle modalitiy files and stacked modalities as different numpy arrays for each. \n","\n","**PS. Note that here this data is preprocessed for autoencoders which were trained on the unlabeled data thus no y labels were availabel and wern't saved.\n","The data is also just used for autoencoder training so there is no train test split performed in the above codes. \n","If a split is needed to be performed it can be simply done by using sklearn train_test_split. Otherwise Midline_imageprep documentation can also be refered for the steps.** "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Way5-wxDXjwI"},"outputs":[],"source":["np.save('../Data/X_train.npy',x_train)\n","np.save('../Data/Y_train.npy',y_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iofA7nB_XjwI"},"outputs":[],"source":["# separating modalitites\n","\n","x_train_FLAIR = x_train[np.arange(0,2025,3),:,:,0]\n","x_train_T1ce = x_train[np.arange(1,2025,3),:,:,0]\n","x_train_T2 = x_train[np.arange(2,2025,3),:,:,0]\n","\n","#x_test_FLAIR = x_test[np.arange(0,171,3),:,:,0]\n","#x_test_T1ce = x_test[np.arange(1,171,3),:,:,0]\n","#x_test_T2 = x_test[np.arange(2,171,3),:,:,0]\n","\n","print(x_train_FLAIR.shape,x_train_T1ce.shape, x_train_T2.shape)\n","#print(x_test_FLAIR.shape,x_test_T1ce.shape,x_test_T2.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YGHFu3JxXjwJ"},"outputs":[],"source":["#stacking the modalities to make a 3 channel image with 3 modalities stacked\n","\n","X_train = np.stack([x_train_FLAIR,x_train_T1ce, x_train_T2],axis=-1)\n","#x_test = np.stack([x_test_FLAIR,x_test_T1ce, x_test_T2],axis=-1)\n","print(X_train.shape)#,x_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nYNedYZ5XjwJ"},"outputs":[],"source":["Y_train = y_train[np.arange(0,len(y_train),3)]\n","#y_test = y_test[np.arange(0,len(y_test),3)]\n","print(Y_train.shape)#,y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlUPKDvwXjwJ"},"outputs":[],"source":["np.save('../Data/X_train_stacked.npy',X_train)\n","np.save('../Data/Y_train_stacked.npy',Y_train)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Multitask_dataprep.ipynb","provenance":[]},"kernelspec":{"display_name":"tf2_env","language":"python","name":"tf2_env"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.9"}},"nbformat":4,"nbformat_minor":0}
